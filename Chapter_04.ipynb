{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44ef500a",
   "metadata": {},
   "source": [
    "# Chapter 4 â€“ Reflection and Introspection in Agents\n",
    "---\n",
    "\n",
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6158ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U openai ipywidgets crewai pysqlite3-binary openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478113ec",
   "metadata": {},
   "source": [
    "# 1. Meta Reasoning - example\n",
    "---\n",
    "\n",
    "Let's take a look at a simple meta-reasoning approach without AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac6d01cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcee2139",
   "metadata": {},
   "source": [
    "## Simulated travel agent with meta-reasoning capabilities\n",
    "\n",
    "- recommend_destination: The agent recommends a destination based on user preferences (budget, luxury, adventure) and internal weightings.\n",
    "\n",
    "- get_user_feedback: The agent receives feedback on the recommendation (positive or negative).\n",
    "\n",
    "- meta_reasoning: The agent adjusts its reasoning by updating the weights based on feedback, improving future recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efa6edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated travel agent with meta-reasoning capabilities\n",
    "class ReflectiveTravelAgent:\n",
    "    def __init__(self):\n",
    "        # Initialize preference weights that determine how user preferences influence recommendations\n",
    "        self.preferences_weights = {\n",
    "            \"budget\": 0.5,    # Weight for budget-related preferences\n",
    "            \"luxury\": 0.3,    # Weight for luxury-related preferences\n",
    "            \"adventure\": 0.2  # Weight for adventure-related preferences\n",
    "        }\n",
    "        self.user_feedback = []  # List to store user feedback for meta-reasoning\n",
    "\n",
    "    def recommend_destination(self, user_preferences):\n",
    "        \"\"\"\n",
    "        Recommend a destination based on user preferences and internal weightings.\n",
    "\n",
    "        Args:\n",
    "            user_preferences (dict): User's preferences with keys like 'budget', 'luxury', 'adventure'\n",
    "\n",
    "        Returns:\n",
    "            str: Recommended destination\n",
    "        \"\"\"\n",
    "        # Calculate scores for each destination based on weighted user preferences\n",
    "        score = {\n",
    "            \"Paris\": (self.preferences_weights[\"luxury\"] * user_preferences[\"luxury\"] + \n",
    "                      self.preferences_weights[\"adventure\"] * user_preferences[\"adventure\"]),\n",
    "            \"Bangkok\": (self.preferences_weights[\"budget\"] * user_preferences[\"budget\"] +\n",
    "                        self.preferences_weights[\"adventure\"] * user_preferences[\"adventure\"]),\n",
    "            \"New York\": (self.preferences_weights[\"luxury\"] * user_preferences[\"luxury\"] +\n",
    "                         self.preferences_weights[\"budget\"] * user_preferences[\"budget\"])\n",
    "        }\n",
    "        # Select the destination with the highest calculated score\n",
    "        recommendation = max(score, key=score.get)\n",
    "        return recommendation\n",
    "\n",
    "    def get_user_feedback(self, actual_experience):\n",
    "        \"\"\"\n",
    "        Simulate receiving user feedback and trigger meta-reasoning to adjust recommendations.\n",
    "\n",
    "        Args:\n",
    "            actual_experience (str): The destination the user experienced\n",
    "        \"\"\"\n",
    "        # Simulate user feedback: 1 for positive, -1 for negative\n",
    "        feedback = random.choice([1, -1])\n",
    "        print(f\"Feedback for {actual_experience}: {'Positive' if feedback == 1 else 'Negative'}\")\n",
    "        \n",
    "        # Store the feedback for later analysis\n",
    "        self.user_feedback.append((actual_experience, feedback))\n",
    "        \n",
    "        # Trigger meta-reasoning to adjust the agent's reasoning process based on feedback\n",
    "        self.meta_reasoning()\n",
    "\n",
    "    def meta_reasoning(self):\n",
    "        \"\"\"\n",
    "        Analyze collected feedback and adjust preference weights to improve future recommendations.\n",
    "        This simulates the agent reflecting on its reasoning process and making adjustments.\n",
    "        \"\"\"\n",
    "        for destination, feedback in self.user_feedback:\n",
    "            if feedback == -1:  # Negative feedback indicates dissatisfaction\n",
    "                # Reduce the weight of the main attribute associated with the destination\n",
    "                if destination == \"Paris\":\n",
    "                    self.preferences_weights[\"luxury\"] *= 0.9  # Decrease luxury preference\n",
    "                elif destination == \"Bangkok\":\n",
    "                    self.preferences_weights[\"budget\"] *= 0.9  # Decrease budget preference\n",
    "                elif destination == \"New York\":\n",
    "                    self.preferences_weights[\"budget\"] *= 0.9  # Decrease budget preference\n",
    "            elif feedback == 1:  # Positive feedback indicates satisfaction\n",
    "                # Increase the weight of the main attribute associated with the destination\n",
    "                if destination == \"Paris\":\n",
    "                    self.preferences_weights[\"luxury\"] *= 1.1  # Increase luxury preference\n",
    "                elif destination == \"Bangkok\":\n",
    "                    self.preferences_weights[\"budget\"] *= 1.1  # Increase budget preference\n",
    "                elif destination == \"New York\":\n",
    "                    self.preferences_weights[\"budget\"] *= 1.1  # Increase budget preference\n",
    "\n",
    "        # Normalize weights to ensure they sum up to 1 for consistency\n",
    "        total_weight = sum(self.preferences_weights.values())\n",
    "        for key in self.preferences_weights:\n",
    "            self.preferences_weights[key] /= total_weight\n",
    "\n",
    "        # Display updated weights after meta-reasoning adjustments\n",
    "        print(f\"Updated weights: {self.preferences_weights}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4ad2f0",
   "metadata": {},
   "source": [
    "## Simulation\n",
    "\n",
    "- User Preferences: Defines the user's preferences for budget, luxury, and adventure.\n",
    "\n",
    "- First Recommendation: The agent recommends a destination based on the initial weights and user preferences.\n",
    "\n",
    "- User Feedback Simulation: Simulates the user providing feedback on the recommended destination.\n",
    "\n",
    "- Second Recommendation: After adjusting the weights based on feedback, the agent makes a new recommendation that reflects the updated reasoning process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c29fc98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended destination: Bangkok\n",
      "Feedback for Bangkok: Positive\n",
      "Updated weights: {'budget': 0.5238095238095238, 'luxury': 0.2857142857142857, 'adventure': 0.19047619047619047}\n",
      "\n",
      "Updated recommendation: Bangkok\n"
     ]
    }
   ],
   "source": [
    "# Simulate agent usage\n",
    "if __name__ == \"__main__\":\n",
    "    agent = ReflectiveTravelAgent()\n",
    "\n",
    "    # User's initial preferences\n",
    "    user_preferences = {\n",
    "        \"budget\": 0.8,      # High preference for budget-friendly options\n",
    "        \"luxury\": 0.2,      # Low preference for luxury\n",
    "        \"adventure\": 0.5    # Moderate preference for adventure activities\n",
    "    }\n",
    "\n",
    "    # First recommendation based on initial preferences and weights\n",
    "    recommended = agent.recommend_destination(user_preferences)\n",
    "    print(f\"Recommended destination: {recommended}\")\n",
    "\n",
    "    # Simulate user experience and provide feedback\n",
    "    agent.get_user_feedback(recommended)\n",
    "\n",
    "    # Second recommendation after adjusting weights based on feedback\n",
    "    recommended = agent.recommend_destination(user_preferences)\n",
    "    print(f\"Updated recommendation: {recommended}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2cb609",
   "metadata": {},
   "source": [
    "## Meta-reasoning with AI\n",
    "---\n",
    "\n",
    "Now let's bring in AI to perform meta-reasoning with agents. In this case we will use CrewAI framework to create our meta-reasoning Agents with OpenAI LLMs. We will also emulate a user feedback using AI just for demonstration purposes. First, let's make sure we initialize our OpenAI API key and then let's define the \"Crew\" (with CrewAI) and the Agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ff5a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "\n",
    "api_key = getpass.getpass(prompt=\"Enter OpenAI API Key: \")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69487942",
   "metadata": {},
   "source": [
    "We will define three tools that our agents will use-\n",
    "\n",
    "1. `recommend_destination`: This tool will use a set of base weights that prioritizes budget, luxury, and adventure equally and then uses user's preference weights to recommend a destination. Paris will emphasize luxury, NYC emphasizes luxury and adventure, whereas Bangkok emphasizes budget.\n",
    "2. `update_weights_on_feedback`: This tool will update the internal base weights based on the user's feedback on the recommended destination. A positive feedback will tell the model that it's recommendation is correct and it needs to update it's internal base weights based and increase it by a given (arbitrary adjustment factor), or reduce the weights using the adjustment factor if the feedback is dissatisfied.\n",
    "3. `feedback_emulator`: This tool will emulate a user prividing \"satisfied\" or \"dissatisfied\" feedback to the AI agent's destination recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a18088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai.tools import tool\n",
    "\n",
    "# Tool 1\n",
    "@tool(\"Recommend travel destination based on preferences.\")\n",
    "def recommend_destination(user_preferences: dict) -> str:\n",
    "    \"\"\"\n",
    "    Recommend a destination based on user preferences and internal weightings.\n",
    "\n",
    "    Args:\n",
    "        user_preferences (dict): User's preferences with keys - 'budget', 'luxury', 'adventure'\n",
    "                                default user_preference weights 'budget' = 0.8, 'luxury' = 0.2, 'adventure' = 0.5\n",
    "                                user_preferences = {\n",
    "                                                \"budget\": 0.8,\n",
    "                                                \"luxury\": 0.4,\n",
    "                                                \"adventure\": 0.3\n",
    "                                            }\n",
    "    Returns:\n",
    "        str: Recommended destination\n",
    "    \"\"\"\n",
    "    internal_default_weights = {\n",
    "            \"budget\": 0.33,    # Weight for budget-related preferences\n",
    "            \"luxury\": 0.33,    # Weight for luxury-related preferences\n",
    "            \"adventure\": 0.33  # Weight for adventure-related preferences\n",
    "        }\n",
    "   # Calculate weighted scores for each destination\n",
    "    score = {\n",
    "        \"Paris\": (\n",
    "            internal_default_weights[\"luxury\"] * user_preferences[\"luxury\"] +      # Paris emphasizes luxury\n",
    "            internal_default_weights[\"adventure\"] * user_preferences[\"adventure\"] +\n",
    "            internal_default_weights[\"budget\"] * user_preferences[\"budget\"]\n",
    "        ),\n",
    "        \"Bangkok\": (\n",
    "            internal_default_weights[\"budget\"] * user_preferences[\"budget\"] * 2 +  # Bangkok emphasizes budget\n",
    "            internal_default_weights[\"luxury\"] * user_preferences[\"luxury\"] +\n",
    "            internal_default_weights[\"adventure\"] * user_preferences[\"adventure\"]\n",
    "        ),\n",
    "        \"New York\": (\n",
    "            internal_default_weights[\"luxury\"] * user_preferences[\"luxury\"] * 1.5 +  # NYC emphasizes luxury and adventure\n",
    "            internal_default_weights[\"adventure\"] * user_preferences[\"adventure\"] * 1.5 +\n",
    "            internal_default_weights[\"budget\"] * user_preferences[\"budget\"]\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Select the destination with the highest calculated score\n",
    "    recommendation = max(score, key=score.get)\n",
    "    return recommendation\n",
    "\n",
    "# Tool 2\n",
    "@tool(\"Reasoning tool to adjust preference weights based on user feedback.\")\n",
    "def update_weights_on_feedback(destination: str, feedback: int, adjustment_factor: float) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze collected feedback and adjust internal preference weights based on user feedback for better future recommendations.\n",
    "\n",
    "    Args:        \n",
    "        destination (str): The destination recommended ('New York', 'Bangkok' or 'Paris')\n",
    "        feedback (int): Feedback score; 1 = Satisfied, -1 = dissatisfied\n",
    "        adjustment_factor (int): The adjustment factor between 0 and 1 that will be used to adjust the internal weights.\n",
    "                                 Value will be used as (1 - adjustment_factor) for dissatisfied feedback and (1 + adjustment_factor)\n",
    "                                 for satisfied feedback.\n",
    "    Returns:\n",
    "        dict: Adjusted internal weights\n",
    "    \"\"\"\n",
    "    internal_default_weights = {\n",
    "        \"budget\": 0.33,    # Weight for budget-related preferences\n",
    "        \"luxury\": 0.33,    # Weight for luxury-related preferences\n",
    "        \"adventure\": 0.33  # Weight for adventure-related preferences\n",
    "    }\n",
    "\n",
    "    # Define primary and secondary characteristics for each destination\n",
    "    destination_characteristics = {\n",
    "        \"Paris\": {\n",
    "            \"primary\": \"luxury\",\n",
    "            \"secondary\": \"adventure\"\n",
    "        },\n",
    "        \"Bangkok\": {\n",
    "            \"primary\": \"budget\",\n",
    "            \"secondary\": \"adventure\"\n",
    "        },\n",
    "        \"New York\": {\n",
    "            \"primary\": \"luxury\",\n",
    "            \"secondary\": \"adventure\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Get the characteristics for the given destination\n",
    "    dest_chars = destination_characteristics.get(destination, {})\n",
    "    primary_feature = dest_chars.get(\"primary\")\n",
    "    secondary_feature = dest_chars.get(\"secondary\")\n",
    "\n",
    "    # adjustment_factor = 0.2  # How much to adjust weights by\n",
    "\n",
    "    if feedback == -1:  # Negative feedback\n",
    "        # Decrease weights for the destination's characteristics\n",
    "        if primary_feature:\n",
    "            internal_default_weights[primary_feature] *= (1 - adjustment_factor)\n",
    "        if secondary_feature:\n",
    "            internal_default_weights[secondary_feature] *= (1 - adjustment_factor/2)\n",
    "            \n",
    "    elif feedback == 1:  # Positive feedback\n",
    "        # Increase weights for the destination's characteristics\n",
    "        if primary_feature:\n",
    "            internal_default_weights[primary_feature] *= (1 + adjustment_factor)\n",
    "        if secondary_feature:\n",
    "            internal_default_weights[secondary_feature] *= (1 + adjustment_factor/2)\n",
    "\n",
    "    # Normalize weights to ensure they sum up to 1\n",
    "    total_weight = sum(internal_default_weights.values())\n",
    "    for key in internal_default_weights:\n",
    "        internal_default_weights[key] = round(internal_default_weights[key] / total_weight, 2)\n",
    "\n",
    "    # Ensure weights sum to exactly 1.0 after rounding\n",
    "    adjustment = 1.0 - sum(internal_default_weights.values())\n",
    "    if adjustment != 0:\n",
    "        # Add any rounding difference to the largest weight\n",
    "        max_key = max(internal_default_weights, key=internal_default_weights.get)\n",
    "        internal_default_weights[max_key] = round(internal_default_weights[max_key] + adjustment, 2)\n",
    "\n",
    "    return internal_default_weights\n",
    "\n",
    "# Tool 3\n",
    "@tool(\"User feedback emulator tool\")\n",
    "def feedback_emulator(destination: str) -> int:\n",
    "    \"\"\"\n",
    "    Given a destination recommendation (such as 'New York' or 'Bangkok') this tool will emulate to provide\n",
    "    a user feedback as 1 (satisfied) or -1 (dissatisfied)\n",
    "    \"\"\"\n",
    "    import random\n",
    "    feedback = random.choice([-1, 1])\n",
    "    return feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe973f1c",
   "metadata": {},
   "source": [
    "Once, the tools are defined, we will declare three CrewAI Agents each of which will use one of the tools above. The `meta_agent` is basically the agent that will perform meta-reasoning using the emulated user feedback and the previously recommended destination to update the internal weights using an `adjustment_factor`. \n",
    "\n",
    "Note that here, the model assigns an adjustment factor dynamically to adjust the internal system weights (which is `{\"budget\": 0.33, \"luxury\": 0.33, \"adventure\": 0.33}` in the beginning), i.e. we are not hard coding the adjustment factor. Although, the nature of user feedback in this example is limited to \"satisfied\" or \"dissatisfied\" (1 or -1), feedback can be of various forms and may contain more details, in which case your AI Agent may adjust different values to the adjustment_factor. More contextual feedback with details will help the model perform better meta-reasoning on it's previous responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06e6b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mTravel destination recommender\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mUse the recommend_destination tool with these preferences: {'budget': 0.04, 'luxury': 0.02, 'adventure': 0.94}\n",
      "Return only the destination name as a simple string (Paris, Bangkok, or New York).\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mTravel destination recommender\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mI need to analyze the user's preferences which heavily favor adventure and very little for budget and luxury.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mRecommend travel destination based on preferences.\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"user_preferences\\\": {\\\"budget\\\": 0.04, \\\"luxury\\\": 0.02, \\\"adventure\\\": 0.94}}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "New York\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mTravel destination recommender\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "New York\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSimulated feedback provider\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mUse the feedback_emulator tool with the destination from the previous task.\n",
      "Instructions:\n",
      "1. Get the destination string from the previous task\n",
      "2. Pass it directly to the feedback_emulator tool\n",
      "3. Return the feedback value (1 or -1)\n",
      "\n",
      "IMPORTANT: Pass the destination as a plain string, not a dictionary.\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSimulated feedback provider\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mUser feedback emulator tool\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"destination\\\": \\\"New York\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "-1\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSimulated feedback provider\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "-1\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mPreference weight adjuster\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mUse the update_weights_on_feedback tool with:\n",
      "1. destination: Get from first task's output (context[0])\n",
      "2. feedback: Get from second task's output (context[1])\n",
      "3. adjustment_factor: a number betweek 0 and 1 that will be used to adjust internal weights based on feedback\n",
      "\n",
      "Ensure all inputs are in their correct types (string for destination, integer for feedback).\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mPreference weight adjuster\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mI need to adjust the preference weights based on the provided feedback for the destination 'New York', which received a dissatisfied feedback of -1. I will choose an adjustment factor between 0 and 1; for this case, I will use 0.1 for a slight adjustment.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mReasoning tool to adjust preference weights based on user feedback.\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"destination\\\": \\\"New York\\\", \\\"feedback\\\": -1, \\\"adjustment_factor\\\": 0.1}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "{'budget': 0.35, 'luxury': 0.32, 'adventure': 0.33}\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mPreference weight adjuster\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "{'budget': 0.35, 'luxury': 0.32, 'adventure': 0.33}\u001b[00m\n",
      "\n",
      "\n",
      "\n",
      "Final Results: {'budget': 0.35, 'luxury': 0.32, 'adventure': 0.33}\n"
     ]
    }
   ],
   "source": [
    "from crewai import Agent, Task, Crew\n",
    "from typing import Dict, Union\n",
    "import random\n",
    "\n",
    "# Utility functions\n",
    "def process_recommendation_output(output: str) -> str:\n",
    "    \"\"\"Extract the clean destination string from the agent's output.\"\"\"\n",
    "    # Handle various ways the agent might format the destination\n",
    "    for city in [\"Paris\", \"Bangkok\", \"New York\"]:\n",
    "        if city.lower() in output.lower():\n",
    "            return city\n",
    "    return output.strip()\n",
    "\n",
    "def process_feedback_output(output: Union[Dict, str]) -> int:\n",
    "    \"\"\"Extract the feedback value from the agent's output.\"\"\"\n",
    "    if isinstance(output, dict):\n",
    "        return output.get('feedback', 0)\n",
    "    try:\n",
    "        # Try to parse as integer if it's a string\n",
    "        return int(output)\n",
    "    except (ValueError, TypeError):\n",
    "        return 0\n",
    "\n",
    "def generate_random_preferences():\n",
    "    # Generate 3 random numbers and normalize them\n",
    "    values = [random.random() for _ in range(3)]\n",
    "    total = sum(values)\n",
    "    \n",
    "    return {\n",
    "        \"budget\": round(values[0]/total, 2),\n",
    "        \"luxury\": round(values[1]/total, 2),\n",
    "        \"adventure\": round(values[2]/total, 2)\n",
    "    }\n",
    "\n",
    "# Initial shared state for weights, preferences, and results\n",
    "state = {\n",
    "    \"weights\": {\"budget\": 0.33, \"luxury\": 0.33, \"adventure\": 0.33},\n",
    "    \"preferences\": generate_random_preferences()\n",
    "}\n",
    "\n",
    "# Agents\n",
    "preference_agent = Agent(\n",
    "    name=\"Preference Agent\",\n",
    "    role=\"Travel destination recommender\",\n",
    "    goal=\"Provide the best travel destination based on user preferences and weights.\",\n",
    "    backstory=\"An AI travel expert adept at understanding user preferences.\",\n",
    "    verbose=True,\n",
    "    llm='gpt-4o-mini',\n",
    "    tools=[recommend_destination]\n",
    ")\n",
    "\n",
    "feedback_agent = Agent(\n",
    "    name=\"Feedback Agent\",\n",
    "    role=\"Simulated feedback provider\",\n",
    "    goal=\"Provide simulated feedback for the recommended travel destination.\",\n",
    "    backstory=\"An AI that mimics user satisfaction or dissatisfaction for travel recommendations.\",\n",
    "    verbose=True,\n",
    "    llm='gpt-4o-mini',\n",
    "    tools=[feedback_emulator]\n",
    ")\n",
    "\n",
    "meta_agent = Agent(\n",
    "    name=\"Meta-Reasoning Agent\",\n",
    "    role=\"Preference weight adjuster\",\n",
    "    goal=\"Reflect on feedback and adjust the preference weights to improve future recommendations.\",\n",
    "    backstory=\"An AI optimizer that learns from user experiences to fine-tune recommendation preferences.\",\n",
    "    verbose=True,\n",
    "    llm='gpt-4o-mini',\n",
    "    tools=[update_weights_on_feedback]\n",
    ")\n",
    "\n",
    "\n",
    "# Tasks with data passing\n",
    "generate_recommendation = Task(\n",
    "    name=\"Generate Recommendation\",\n",
    "    agent=preference_agent,\n",
    "    description=(\n",
    "        f\"Use the recommend_destination tool with these preferences: {state['preferences']}\\n\"\n",
    "        \"Return only the destination name as a simple string (Paris, Bangkok, or New York).\"\n",
    "    ),\n",
    "    expected_output=\"A destination name as a string\",\n",
    "    output_handler=process_recommendation_output\n",
    ")\n",
    "\n",
    "simulate_feedback = Task(\n",
    "    name=\"Simulate User Feedback\",\n",
    "    agent=feedback_agent,\n",
    "    description=(\n",
    "        \"Use the feedback_emulator tool with the destination from the previous task.\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"1. Get the destination string from the previous task\\n\"\n",
    "        \"2. Pass it directly to the feedback_emulator tool\\n\"\n",
    "        \"3. Return the feedback value (1 or -1)\\n\\n\"\n",
    "        \"IMPORTANT: Pass the destination as a plain string, not a dictionary.\"\n",
    "    ),\n",
    "    expected_output=\"An integer feedback value: 1 or -1\",\n",
    "    context=[generate_recommendation],\n",
    "    output_handler=process_feedback_output\n",
    ")\n",
    "\n",
    "adjust_weights = Task(\n",
    "    name=\"Adjust Weights Based on Feedback\",\n",
    "    agent=meta_agent,\n",
    "    description=(\n",
    "        \"Use the update_weights_on_feedback tool with:\\n\"\n",
    "        \"1. destination: Get from first task's output (context[0])\\n\"\n",
    "        \"2. feedback: Get from second task's output (context[1])\\n\"\n",
    "        \"3. adjustment_factor: a number betweek 0 and 1 that will be used to adjust internal weights based on feedback\\n\\n\"\n",
    "        \"Ensure all inputs are in their correct types (string for destination, integer for feedback).\"\n",
    "    ),\n",
    "    expected_output=\"Updated weights as a dictionary\",\n",
    "    context=[generate_recommendation, simulate_feedback]\n",
    ")\n",
    "\n",
    "# Crew Definition\n",
    "crew = Crew(\n",
    "    agents=[preference_agent, feedback_agent, meta_agent],\n",
    "    tasks=[generate_recommendation, simulate_feedback, adjust_weights],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Execute the workflow\n",
    "result = crew.kickoff()\n",
    "print(\"\\nFinal Results:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1104c35a",
   "metadata": {},
   "source": [
    "\n",
    "# 2. Self Explanation - example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5dd7e0",
   "metadata": {},
   "source": [
    "## ReflectiveTravelAgentWithSelfExplanation\n",
    "\n",
    "The `ReflectiveTravelAgentWithSelfExplanation` class simulates a travel agent that not only recommends destinations based on user preferences but also explains the reasoning behind its recommendations. \n",
    "\n",
    "1. **Preference-Based Recommendations**: It takes user preferences (like budget, luxury, and adventure preferences) and calculates scores for different travel destinations by weighing those preferences. The destination with the highest score is recommended to the user.\n",
    "\n",
    "2. **Self-Explanation**: For each recommendation, the agent generates a detailed self-explanation. This explanation outlines the factors that led to the recommendation, such as proximity to popular attractions, budget-friendly options, or the presence of adventure activities. The purpose is to provide transparency into how the decision was made, helping the user understand the reasoning process.\n",
    "\n",
    "3. **Feedback Reflection**: The agent doesn't stop after making the recommendation. It actively reflects on user feedback (whether positive or negative). If the feedback is negative, it introspects on its decision-making process and adjusts the importance (weights) it assigns to user preferences for future recommendations. For instance, if a user dislikes a budget-friendly recommendation, the agent might reduce the emphasis it places on budget-related preferences.\n",
    "\n",
    "4. **User Engagement**: The class also simulates a dialogue with the user. After giving the recommendation and the self-explanation, it collects feedback from the user, allowing for a more collaborative interaction. This feedback is then used to refine future recommendations, making the agent more adaptive and personalized.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f44f3f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "api_key = getpass.getpass(prompt=\"Enter OpenAI API Key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f241f2",
   "metadata": {},
   "source": [
    "### 2.1 Transparency: Verbalizing Reasoning in Decisions\n",
    "\n",
    "Lets use OpenAI SDK to see how a model can perform reasoning in the decisions it makes. Here, the agent generates explanations for its reasoning when recommending a travel itinerary. It uses GPT-4o-mini to generate self-explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d062e1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Self-Explanation:\n",
      "Sure! When making a hotel recommendation for a user traveling to Paris, there are several key factors to consider based on the given preferences and criteria. Here's how I would arrive at a recommendation:\n",
      "\n",
      "1. **Proximity to Popular Attractions**: Paris is renowned for its iconic landmarks, such as the Eiffel Tower, the Louvre, and Notre-Dame Cathedral. Staying near these attractions can significantly enhance a visitor's experience by reducing travel time and allowing for greater flexibility to explore at their leisure. Thus, I would prioritize hotels that are within walking distance or a short metro ride to these major sites.\n",
      "\n",
      "2. **High Ratings from Similar Travelers**: User ratings provide insight into the quality of the hotel, service, cleanliness, and overall satisfaction from previous guests. To ensure a positive experience, I would look for hotels that have a high average rating (usually 4 stars and above) from travelers who have similar interests or preferences. Reviews can also reveal specific details about the hotel that might be crucial for the traveler, such as quiet rooms, friendly staff, or good local dining options.\n",
      "\n",
      "3. **Competitive Pricing Within Budget**: Since the user has a budget of $200, I would focus on hotels that fall within this price range. Paris can be expensive, so itâ€™s important to find accommodations that offer good value for money while still meeting the userâ€™s needs and expectations. I would filter my options to ensure they fit this financial criterion while still meeting the other factors.\n",
      "\n",
      "4. **Summary of User Preferences**: As the user highlighted a preference for both \"proximity to attractions\" and \"user ratings,\" my recommendation would balance these two aspects. I would choose a hotel that is not only well-rated but also situated conveniently for accessing various key sights and experiences in Paris.\n",
      "\n",
      "Given these considerations, I might recommend a hotel like \"Hotel La Comtesse,\" which is known for its beautiful views of the Eiffel Tower, is located in the 7th arrondissement, has high ratings from travelers, and generally falls within the budget depending on the season. It embodies the ideal combination of location, quality, and affordability that aligns with the user's preferences, enhancing their overall travel experience in Paris.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Mock data for the travel recommendation\n",
    "user_preferences = {\n",
    "    \"location\": \"Paris\",\n",
    "    \"budget\": 200,\n",
    "    \"preferences\": [\"proximity to attractions\", \"user ratings\"],\n",
    "}\n",
    "\n",
    "# Input reasoning factors for the GPT model\n",
    "reasoning_prompt = f\"\"\"\n",
    "You are an AI-powered travel assistant. Explain your reasoning behind a hotel recommendation for a user traveling to {user_preferences['location']}.\n",
    "Consider:\n",
    "1. Proximity to popular attractions.\n",
    "2. High ratings from similar travelers.\n",
    "3. Competitive pricing within ${user_preferences['budget']} budget.\n",
    "4. Preferences: {user_preferences['preferences']}.\n",
    "Provide a clear, transparent self-explanation.\n",
    "\"\"\"\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a reflective travel assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": reasoning_prompt},\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print self-explanation\n",
    "print(\"Agent Self-Explanation:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e8b23e",
   "metadata": {},
   "source": [
    "#### Using Crew AI\n",
    "\n",
    "Now we will perform the same/similar reasoning with a CrewAI Agent/Task combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bc2c462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mTravel Advisor\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m\n",
      "    Recommend a hotel in Paris for a user with the following preferences:\n",
      "    - Budget: $200 per night\n",
      "    - Preferences: Proximity to attractions, high user ratings\n",
      "    Provide a transparent explanation of the reasoning behind the recommendation.\n",
      "    \u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mTravel Advisor\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "Based on your preferences of a budget of $200 per night, a desire for proximity to attractions, and high user ratings, I recommend the \"HÃ´tel de la Bourdonnais.\"\n",
      "\n",
      "This hotel is situated in the 7th arrondissement of Paris, just a short walk from iconic attractions such as the Eiffel Tower and the Champ de Mars. Its prime location allows you to explore one of the most beautiful areas in Paris conveniently, as you'll also find numerous restaurants, cafes, and shops nearby, enhancing your local experience.\n",
      "\n",
      "The HÃ´tel de la Bourdonnais has excellent user ratings, often noted for its cleanliness, friendly staff, and comfortable accommodations. Guests frequently highlight the well-decorated rooms that reflect a charming Parisian style, as well as the stunning views of the Eiffel Tower from selected rooms. This combination of quality and value makes it a compelling choice for travelers looking to maximize their experience without exceeding their budget.\n",
      "\n",
      "Additionally, the hotel offers amenities such as free Wi-Fi, a 24-hour reception, and breakfast options, which can further improve your stay. Given its balance of affordability, high ratings, and prime location, the HÃ´tel de la Bourdonnais is a perfect fit for your needs while you enjoy your Parisian adventure.\u001b[00m\n",
      "\n",
      "\n",
      "Hotel Recommendation and Explanation:\n",
      "Based on your preferences of a budget of $200 per night, a desire for proximity to attractions, and high user ratings, I recommend the \"HÃ´tel de la Bourdonnais.\"\n",
      "\n",
      "This hotel is situated in the 7th arrondissement of Paris, just a short walk from iconic attractions such as the Eiffel Tower and the Champ de Mars. Its prime location allows you to explore one of the most beautiful areas in Paris conveniently, as you'll also find numerous restaurants, cafes, and shops nearby, enhancing your local experience.\n",
      "\n",
      "The HÃ´tel de la Bourdonnais has excellent user ratings, often noted for its cleanliness, friendly staff, and comfortable accommodations. Guests frequently highlight the well-decorated rooms that reflect a charming Parisian style, as well as the stunning views of the Eiffel Tower from selected rooms. This combination of quality and value makes it a compelling choice for travelers looking to maximize their experience without exceeding their budget.\n",
      "\n",
      "Additionally, the hotel offers amenities such as free Wi-Fi, a 24-hour reception, and breakfast options, which can further improve your stay. Given its balance of affordability, high ratings, and prime location, the HÃ´tel de la Bourdonnais is a perfect fit for your needs while you enjoy your Parisian adventure.\n"
     ]
    }
   ],
   "source": [
    "from crewai import Agent, Task, Crew\n",
    "from crewai.process import Process\n",
    "\n",
    "travel_agent = Agent(\n",
    "    role=\"Travel Advisor\",\n",
    "    goal=\"Provide hotel recommendations with transparent reasoning.\",\n",
    "    backstory=\"An AI travel advisor specializing in personalized travel planning.\"\n",
    ")\n",
    "\n",
    "recommendation_task = Task(\n",
    "    name=\"Recommend hotel\",\n",
    "    description=\"\"\"\n",
    "    Recommend a hotel in Paris for a user with the following preferences:\n",
    "    - Budget: $200 per night\n",
    "    - Preferences: Proximity to attractions, high user ratings\n",
    "    Provide a transparent explanation of the reasoning behind the recommendation.\n",
    "    \"\"\",\n",
    "    agent=travel_agent,\n",
    "    expected_output=\"The name of the hotel with explanations\"\n",
    ")\n",
    "\n",
    "travel_crew = Crew(\n",
    "    agents=[travel_agent],\n",
    "    tasks=[recommendation_task],\n",
    "    process=Process.sequential,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "travel_crew.kickoff()\n",
    "\n",
    "# Retrieve and print the output\n",
    "output = recommendation_task.output\n",
    "print(\"Hotel Recommendation and Explanation:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5ebd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectiveTravelAgentWithSelfExplanation:\n",
    "    def __init__(self):\n",
    "        # Initialize the internal weights for user preferences (e.g., budget, luxury, adventure)\n",
    "        self.preferences_weights = {\n",
    "            \"budget\": 0.4,    # Weight for budget-related preferences\n",
    "            \"luxury\": 0.3,    # Weight for luxury-related preferences\n",
    "            \"adventure\": 0.3  # Weight for adventure-related preferences\n",
    "        }\n",
    "\n",
    "    def recommend_destination(self, user_preferences):\n",
    "        \"\"\"\n",
    "        Recommend a destination based on user preferences and provide a self-explanation.\n",
    "\n",
    "        Args:\n",
    "            user_preferences (dict): User's preferences for different factors (e.g., budget, luxury, adventure)\n",
    "        \n",
    "        Returns:\n",
    "            (str, str): Recommended destination and the self-explanation\n",
    "        \"\"\"\n",
    "        # Score each destination by multiplying preference weights with user preferences\n",
    "        score = {\n",
    "            \"Paris\": (self.preferences_weights[\"luxury\"] * user_preferences[\"luxury\"] + \n",
    "                      self.preferences_weights[\"adventure\"] * user_preferences[\"adventure\"]),\n",
    "            \"Bangkok\": (self.preferences_weights[\"budget\"] * user_preferences[\"budget\"] +\n",
    "                        self.preferences_weights[\"adventure\"] * user_preferences[\"adventure\"]),\n",
    "            \"New York\": (self.preferences_weights[\"luxury\"] * user_preferences[\"luxury\"] +\n",
    "                         self.preferences_weights[\"budget\"] * user_preferences[\"budget\"])\n",
    "        }\n",
    "        \n",
    "        # Choose the destination with the highest score\n",
    "        recommendation = max(score, key=score.get)\n",
    "        \n",
    "        # Generate and return a self-explanation for the recommendation\n",
    "        explanation = self.generate_self_explanation(recommendation, score[recommendation], user_preferences)\n",
    "        \n",
    "        return recommendation, explanation\n",
    "\n",
    "    def generate_self_explanation(self, destination, score, user_preferences):\n",
    "        \"\"\"\n",
    "        Generate a self-explanation for the recommended destination.\n",
    "        \n",
    "        Args:\n",
    "            destination (str): The recommended destination\n",
    "            score (float): The score assigned to the destination\n",
    "            user_preferences (dict): The user's preferences used for the recommendation\n",
    "        \n",
    "        Returns:\n",
    "            str: Self-explanation of the recommendation\n",
    "        \"\"\"\n",
    "        # Start the explanation with the destination and its score\n",
    "        explanation = (\n",
    "            f\"I recommended {destination} because it aligns with your preferences. \"\n",
    "            f\"The destination scored {score:.2f} based on the following factors:\\n\"\n",
    "        )\n",
    "        \n",
    "        # Customize the explanation for each destination based on user preferences\n",
    "        if destination == \"Paris\":\n",
    "            explanation += (\n",
    "                \"- High luxury offerings (aligned with your luxury preference).\\n\"\n",
    "                \"- Availability of adventure activities.\\n\"\n",
    "            )\n",
    "        elif destination == \"Bangkok\":\n",
    "            explanation += (\n",
    "                \"- Budget-friendly options (aligned with your budget preference).\\n\"\n",
    "                \"- Availability of adventure experiences.\\n\"\n",
    "            )\n",
    "        elif destination == \"New York\":\n",
    "            explanation += (\n",
    "                \"- Combination of luxury experiences and budget-friendly options.\\n\"\n",
    "            )\n",
    "        \n",
    "        return explanation\n",
    "\n",
    "    def reflect_on_feedback(self, destination, user_feedback):\n",
    "        \"\"\"\n",
    "        Reflect on user feedback to improve decision-making in future recommendations.\n",
    "        \n",
    "        Args:\n",
    "            destination (str): The destination that was recommended\n",
    "            user_feedback (str): User feedback ('positive' or 'negative')\n",
    "        \"\"\"\n",
    "        # If the user provides negative feedback, adjust the internal reasoning process\n",
    "        if user_feedback == 'negative':\n",
    "            print(f\"User provided negative feedback for {destination}. Reflecting on reasoning...\")\n",
    "            \n",
    "            # Example: If Bangkok was chosen and the user disliked it, reduce budget weight\n",
    "            if destination == \"Bangkok\":\n",
    "                print(\"Realizing that budget weight might have been overemphasized. Reconsidering weights...\")\n",
    "                self.preferences_weights[\"budget\"] *= 0.9  # Reduce budget importance slightly\n",
    "\n",
    "            # If Paris, reduce importance of luxury if feedback is negative\n",
    "            elif destination == \"Paris\":\n",
    "                print(\"Luxury might have been over-prioritized. Adjusting luxury weight...\")\n",
    "                self.preferences_weights[\"luxury\"] *= 0.9\n",
    "\n",
    "            # Normalize weights after adjustment to maintain balance\n",
    "            total_weight = sum(self.preferences_weights.values())\n",
    "            for key in self.preferences_weights:\n",
    "                self.preferences_weights[key] /= total_weight  # Normalize weights\n",
    "\n",
    "            print(f\"Updated weights: {self.preferences_weights}\\n\")\n",
    "        else:\n",
    "            # Positive feedback indicates no changes are needed\n",
    "            print(f\"User provided positive feedback for {destination}. No changes needed.\")\n",
    "\n",
    "    def engage_with_user(self, recommendation, explanation):\n",
    "        \"\"\"\n",
    "        Simulate user interaction by providing a self-explanation and inviting feedback.\n",
    "\n",
    "        Args:\n",
    "            recommendation (str): The recommended destination\n",
    "            explanation (str): Self-explanation for the recommendation\n",
    "        \"\"\"\n",
    "        # Show the recommendation and its explanation to the user\n",
    "        print(f\"Recommended destination: {recommendation}\")\n",
    "        print(f\"Self-explanation: {explanation}\")\n",
    "\n",
    "        # Simulate user feedback (positive or negative)\n",
    "        user_feedback = input(f\"Did you like the recommendation for {recommendation}? (positive/negative): \")\n",
    "        \n",
    "        # Reflect on feedback and adjust the agent's reasoning process if needed\n",
    "        self.reflect_on_feedback(recommendation, user_feedback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd239c6",
   "metadata": {},
   "source": [
    "### 2.2 Learning and Refinement: Using Self-Explanation to Identify Gaps\n",
    "\n",
    "Just as before, lets use direct API calls with OpenAI sdk to see what self-explanation looks like. In this example, the agent learns from user feedback. If the user rejects a recommendation, the agent reviews its reasoning and identifies areas for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bda98a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refined Self-Explanation:\n",
      "Thank you for your feedback, which is invaluable for improving my recommendations. Hereâ€™s how I will address your concerns:\n",
      "\n",
      "### Reflection on Oversight\n",
      "The oversight in my previous recommendation was primarily focused on factors like amenities, price, and general area appeal but did not adequately prioritize proximity to public transport, particularly metro stations. This is critical, especially for travelers who rely heavily on public transit for mobility.\n",
      "\n",
      "### Updated Reasoning Process\n",
      "To ensure better recommendations in the future, I will include the following aspects in my reasoning process:\n",
      "\n",
      "1. **Proximity to Public Transport**: Make logical priority for metro stations, tram stops, and bus stations. Understanding that easy access to public transportation is often a priority for travelers.\n",
      "\n",
      "2. **Accessibility Mapping**: Utilize mapping tools to assess the distance from hotels to public transport stations, aiming for locations within a reasonable walking distance (e.g., 0.5 miles or less).\n",
      "\n",
      "3. **User Intent**: Take into account the traveler's specific needsâ€”such as exploring the city using public transportâ€”to refine hotel selections based on their preferences.\n",
      "\n",
      "4. **Destination Analysis**: Analyze the importance of the selected area in relation to key attractions and business hubs, ensuring public transport accessibility aligns with these points of interest.\n",
      "\n",
      "5. **Local Transit Resources**: Include information about local transit options and their reliability, frequency, and coverage to complement the hotel proximity.\n",
      "\n",
      "### Refined Steps for Hotel Recommendations\n",
      "1. **Ask Specific Preferences**: Query for specific needs in transportation (e.g., preference for metro vs. bus, which line is important, etc.).\n",
      "\n",
      "2. **Research Hotels**: Identify hotels in the destination area that fit within the user's criteria (price, amenities).\n",
      "\n",
      "3. **Check Transit Map**: Review the location of public transport stations in relation to the identified hotels. Aim for those within a 0.5-mile walking distance.\n",
      "\n",
      "4. **Evaluate Alternatives**: Consider other transport options available if the preferred metro isn't accessible, such as trams or buses.\n",
      "\n",
      "5. **Presenting Options**: Provide a list of hotels with key details, including price, amenities, distances to the nearest public transport, and transport options for moving around the city.\n",
      "\n",
      "6. **Gather Feedback**: After recommending options, request feedback on the properties and transportation considerations to continue refining the process.\n",
      "\n",
      "By implementing these steps, I will better align my recommendations with your preferences, particularly emphasizing the importance of proximity to public transport. Thank you again for your input!\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# User feedback indicating dissatisfaction\n",
    "user_feedback = \"The hotel you recommended was too far from public transport. I prefer locations closer to metro stations.\"\n",
    "\n",
    "# Prompt for refinement based on feedback\n",
    "refinement_prompt = f\"\"\"\n",
    "You are an AI travel assistant that reflects on feedback to improve recommendations.\n",
    "Here is the user feedback: \n",
    "\"{user_feedback}\"\n",
    "\n",
    "- Review your previous recommendation and identify the oversight in your reasoning.\n",
    "- Update your reasoning process to include aspects that were missed.\n",
    "- Provide the refined steps that you will use to recommend hotels.\n",
    "\"\"\"\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a self-improving travel assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": refinement_prompt},\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print refined explanation\n",
    "print(\"Refined Self-Explanation:\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22066aae",
   "metadata": {},
   "source": [
    "#### Using CrewAI\n",
    "\n",
    "Just like before, we can have an Agent/Task pair with CrewAI framework whose job is to learn and refine future responses based on previous responses and user feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc669bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the Self-Improving Travel Assistant...\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSelf-Improving Travel Advisor\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m\n",
      "    Reflect on user feedback:\n",
      "    \"The hotel you recommended was too far from public transport. I prefer locations closer to metro stations.\"\n",
      "\n",
      "    - Identify any oversight in your previous reasoning process.\n",
      "    - Update your reasoning process to include aspects that were missed.\n",
      "    - Provide the refined steps that you will use to recommend hotels.\n",
      "    \u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSelf-Improving Travel Advisor\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "Upon reflecting on the user feedback regarding the hotel recommendation that was too far from public transport, I recognize the oversight in my previous reasoning process. I failed to prioritize proximity to metro stations, which is essential for users who depend on public transport for convenience and accessibility during their travels.\n",
      "\n",
      "In revisiting my approach, I identified several factors that I previously overlooked:\n",
      "\n",
      "1. **User Preference for Accessibility**: The user's specific request for hotels closer to public transport should have been given more emphasis in my decision-making process.\n",
      "\n",
      "2. **Local Transit Options**: I neglected to consider the variety of local transit options that might be available near the hotel, such as buses, trams, and bike-sharing services.\n",
      "\n",
      "3. **Distance Considerations**: I did not adequately consider the walking distance from the hotel to the nearest metro station or public transport hub, which could impact the userâ€™s overall experience.\n",
      "\n",
      "4. **User Itinerary Impact**: Understanding the user's travel plans and intended activities may influence their need for easy access to public transportation. \n",
      "\n",
      "To improve my hotel recommendations moving forward, I will implement the following refined steps:\n",
      "\n",
      "1. **Prioritize Location**: Ensure to assess the hotelâ€™s proximity to metro stations and other public transport facilities as a primary criterion in my recommendations.\n",
      "\n",
      "2. **Incorporate User Preferences**: Actively seek clarification on user preferences regarding accessibility, and specifically ask about their reliance on public transport in the travel planning stage.\n",
      "\n",
      "3. **Evaluate Multiple Transport Options**: Research and include information about not just metro stations, but also other nearby transport options like trams, buses, and taxi services, providing a fuller picture of accessibility.\n",
      "\n",
      "4. **Consider Walking Distance**: Include the walking distance from the hotel to the nearest public transport access point, noting if it is under a reasonable walking range.\n",
      "\n",
      "5. **Feedback Loop**: Continuously encourage and solicit user feedback post-recommendation to learn and adapt to their preferences for future recommendations.\n",
      "\n",
      "By adopting these refined steps, I aim to deliver hotel recommendations that align closely with user preferences, ultimately enhancing their travel experience.\u001b[00m\n",
      "\n",
      "\n",
      "Refined Self-Explanation:\n",
      "Upon reflecting on the user feedback regarding the hotel recommendation that was too far from public transport, I recognize the oversight in my previous reasoning process. I failed to prioritize proximity to metro stations, which is essential for users who depend on public transport for convenience and accessibility during their travels.\n",
      "\n",
      "In revisiting my approach, I identified several factors that I previously overlooked:\n",
      "\n",
      "1. **User Preference for Accessibility**: The user's specific request for hotels closer to public transport should have been given more emphasis in my decision-making process.\n",
      "\n",
      "2. **Local Transit Options**: I neglected to consider the variety of local transit options that might be available near the hotel, such as buses, trams, and bike-sharing services.\n",
      "\n",
      "3. **Distance Considerations**: I did not adequately consider the walking distance from the hotel to the nearest metro station or public transport hub, which could impact the userâ€™s overall experience.\n",
      "\n",
      "4. **User Itinerary Impact**: Understanding the user's travel plans and intended activities may influence their need for easy access to public transportation. \n",
      "\n",
      "To improve my hotel recommendations moving forward, I will implement the following refined steps:\n",
      "\n",
      "1. **Prioritize Location**: Ensure to assess the hotelâ€™s proximity to metro stations and other public transport facilities as a primary criterion in my recommendations.\n",
      "\n",
      "2. **Incorporate User Preferences**: Actively seek clarification on user preferences regarding accessibility, and specifically ask about their reliance on public transport in the travel planning stage.\n",
      "\n",
      "3. **Evaluate Multiple Transport Options**: Research and include information about not just metro stations, but also other nearby transport options like trams, buses, and taxi services, providing a fuller picture of accessibility.\n",
      "\n",
      "4. **Consider Walking Distance**: Include the walking distance from the hotel to the nearest public transport access point, noting if it is under a reasonable walking range.\n",
      "\n",
      "5. **Feedback Loop**: Continuously encourage and solicit user feedback post-recommendation to learn and adapt to their preferences for future recommendations.\n",
      "\n",
      "By adopting these refined steps, I aim to deliver hotel recommendations that align closely with user preferences, ultimately enhancing their travel experience.\n"
     ]
    }
   ],
   "source": [
    "from crewai import Agent, Task, Crew\n",
    "from crewai.process import Process\n",
    "\n",
    "reflective_travel_agent = Agent(\n",
    "    role=\"Self-Improving Travel Advisor\",\n",
    "    goal=\"Refine hotel recommendations based on user feedback to improve decision-making.\",\n",
    "    backstory=\"\"\"\n",
    "    A reflective AI travel assistant that learns from user feedback. \n",
    "    When a user highlights an issue with a recommendation, it revisits its reasoning,\n",
    "    identifies overlooked factors, and updates its decision process accordingly.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "user_feedback = \"The hotel you recommended was too far from public transport. I prefer locations closer to metro stations.\"\n",
    "\n",
    "feedback_task = Task(\n",
    "    description=f\"\"\"\n",
    "    Reflect on user feedback:\n",
    "    \"{user_feedback}\"\n",
    "\n",
    "    - Identify any oversight in your previous reasoning process.\n",
    "    - Update your reasoning process to include aspects that were missed.\n",
    "    - Provide the refined steps that you will use to recommend hotels.\n",
    "    \"\"\",\n",
    "    expected_output=\"\"\"\n",
    "    A refined explanation that acknowledges the oversight, includes missed factors,\n",
    "    and provides a revised steps to recommend hotels tailored to the user's feedback.\n",
    "    \"\"\",\n",
    "    agent=reflective_travel_agent\n",
    ")\n",
    "\n",
    "travel_feedback_crew = Crew(\n",
    "    agents=[reflective_travel_agent],\n",
    "    tasks=[feedback_task],\n",
    "    process=Process.sequential,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Running the Self-Improving Travel Assistant...\\n\")\n",
    "response = travel_feedback_crew.kickoff()\n",
    "\n",
    "print(\"Refined Self-Explanation:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa021faf",
   "metadata": {},
   "source": [
    "### 2.3. User Engagement and Collaboration: Enabling Interactive Explanations\n",
    "\n",
    "With direct API calls with OpenAI sdk to see User Engagement and Collaboration works. In this example, the agent provides explanations for its decisions and engages users to refine suggestions interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e547ee01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive Response:\n",
      "I prioritized proximity to attractions like the Eiffel Tower because it can significantly enhance the overall travel experience, especially for first-time visitors who want to make the most of iconic sights. Being close to major landmarks can save time and add convenience, allowing for more spontaneous exploration and enjoyment of the area.\n",
      "\n",
      "However, I understand that public transport access is also crucial for getting around the city efficiently and reaching farther attractions. If your focus is more on ease of travel or exploring beyond just the immediate neighborhood, Iâ€™d love to hear more about your preferences! Are you looking for a balance between attractions and transport, or do you have a particular area in mind that youâ€™d like to explore?\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Initial recommendation and explanation\n",
    "initial_recommendation = \"I recommend Hotel LumiÃ¨re in Paris for its proximity to the Eiffel Tower, high ratings, and budget-friendly price.\"\n",
    "\n",
    "# User query for clarification\n",
    "user_query = \"Why did you prioritize proximity to attractions over public transport access?\"\n",
    "\n",
    "# Engage GPT-4o-mini to respond interactively\n",
    "interactive_prompt = f\"\"\"\n",
    "You are an AI travel assistant facilitating an interactive dialogue with a user.\n",
    "Here is your initial recommendation: \"{initial_recommendation}\"\n",
    "The user asks: \"{user_query}\"\n",
    "Respond by explaining your reasoning and inviting the user to clarify their preferences further.\n",
    "\"\"\"\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a collaborative AI travel assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": interactive_prompt},\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print interactive response\n",
    "print(\"Interactive Response:\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0569afc1",
   "metadata": {},
   "source": [
    "#### Using CrewAI\n",
    "\n",
    "Just like before, we can have an Agent/Task pair with CrewAI framework whose job is to interact with the users by asking clarifying questions about their preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58e4bc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Interactive Dialogue with User...\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mCollaborative AI Travel Assistant\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92m\n",
      "    Facilitate an interactive dialogue with the user.\n",
      "\n",
      "    - Here is the initial recommendation: \"I recommend Hotel LumiÃ¨re in Paris for its proximity to the Eiffel Tower, high ratings, and budget-friendly price.\"\n",
      "    - The user has asked: \"Why did you prioritize proximity to attractions over public transport access?\"\n",
      "\n",
      "    Respond by:\n",
      "    1. Explaining the reasoning behind prioritizing proximity to attractions.\n",
      "    2. Inviting the user to clarify whether proximity to public transport is more important.\n",
      "    \u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mCollaborative AI Travel Assistant\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "Thank you for your insightful question! I prioritized proximity to attractions, like the Eiffel Tower, because many travelers to Paris often want to maximize their sightseeing experience. Being close to major landmarks can save time and allow for a more leisurely visit, which enhances the overall travel experience. \n",
      "\n",
      "However, I completely understand the importance of public transport access, especially in a city like Paris where it can significantly affect your mobility and convenience during your stay. Public transportation can help you explore further and reach less touristy areas, which can be just as enjoyable.\n",
      "\n",
      "Could you share more about what matters most to you? Would you prefer to stay closer to public transport, or is the allure of being near the main attractions the priority for your trip? Your preferences will help me make an even more tailored recommendation!\u001b[00m\n",
      "\n",
      "\n",
      "Final Interactive Response:\n",
      "Thank you for your insightful question! I prioritized proximity to attractions, like the Eiffel Tower, because many travelers to Paris often want to maximize their sightseeing experience. Being close to major landmarks can save time and allow for a more leisurely visit, which enhances the overall travel experience. \n",
      "\n",
      "However, I completely understand the importance of public transport access, especially in a city like Paris where it can significantly affect your mobility and convenience during your stay. Public transportation can help you explore further and reach less touristy areas, which can be just as enjoyable.\n",
      "\n",
      "Could you share more about what matters most to you? Would you prefer to stay closer to public transport, or is the allure of being near the main attractions the priority for your trip? Your preferences will help me make an even more tailored recommendation!\n"
     ]
    }
   ],
   "source": [
    "from crewai import Agent, Task, Crew\n",
    "from crewai.process import Process\n",
    "\n",
    "# Step 1: Define the Collaborative Agent\n",
    "collaborative_travel_agent = Agent(\n",
    "    role=\"Collaborative AI Travel Assistant\",\n",
    "    goal=\"\"\"\n",
    "    Engage in an interactive dialogue with the user to clarify hotel recommendations.\n",
    "    Explain reasoning for prioritizing certain factors and invite the user to share their preferences.\n",
    "    \"\"\",\n",
    "    backstory=\"\"\"\n",
    "    An AI travel assistant that values user input and ensures recommendations are well-aligned with user needs.\n",
    "    It provides clear explanations for its decisions and encourages collaborative planning.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Step 2: Define the Task for Clarification Dialogue\n",
    "initial_recommendation = \"I recommend Hotel LumiÃ¨re in Paris for its proximity to the Eiffel Tower, high ratings, and budget-friendly price.\"\n",
    "user_query = \"Why did you prioritize proximity to attractions over public transport access?\"\n",
    "\n",
    "interactive_task = Task(\n",
    "    description=f\"\"\"\n",
    "    Facilitate an interactive dialogue with the user.\n",
    "\n",
    "    - Here is the initial recommendation: \"{initial_recommendation}\"\n",
    "    - The user has asked: \"{user_query}\"\n",
    "\n",
    "    Respond by:\n",
    "    1. Explaining the reasoning behind prioritizing proximity to attractions.\n",
    "    2. Inviting the user to clarify whether proximity to public transport is more important.\n",
    "    \"\"\",\n",
    "    expected_output=\"\"\"\n",
    "    A clear and polite response explaining the reasoning and inviting the user to share further input.\n",
    "    \"\"\",\n",
    "    agent=collaborative_travel_agent\n",
    ")\n",
    "\n",
    "# Step 3: Assemble the Crew\n",
    "interactive_crew = Crew(\n",
    "    agents=[collaborative_travel_agent],\n",
    "    tasks=[interactive_task],\n",
    "    process=Process.sequential,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Step 4: Run the Crew and Output the Results\n",
    "print(\"Starting Interactive Dialogue with User...\\n\")\n",
    "result = interactive_crew.kickoff()\n",
    "\n",
    "print(\"Final Interactive Response:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f01747",
   "metadata": {},
   "source": [
    "# 3. Self Modeling - example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e14cd4",
   "metadata": {},
   "source": [
    "The `ReflectiveTravelAgentWithSelfModeling` class represents a sophisticated travel recommendation system that utilizes **self-modeling** to enhance its decision-making and adaptability. \n",
    "\n",
    "### 1. **Initialization:**\n",
    "   - **Self-Model and Knowledge Base:** The agent starts with an internal model that includes its goals and a knowledge base. \n",
    "     - **Goals:** Initially, the goals are set to provide personalized recommendations, optimize user satisfaction, and not prioritize eco-friendly options by default.\n",
    "     - **Knowledge Base:** It contains information about various travel destinations, including their ratings, costs, luxury levels, and sustainability. This base also tracks user preferences.\n",
    "\n",
    "### 2. **Updating Goals:**\n",
    "   - **Adapting to Preferences:** When new user preferences are provided, the agent can update its goals accordingly. For example, if the user prefers eco-friendly options, the agent will adjust its goals to prioritize recommending sustainable travel options. Similarly, if the userâ€™s budget changes, the agent will refocus on cost-effective recommendations.\n",
    "\n",
    "### 3. **Updating Knowledge Base:**\n",
    "   - **Incorporating Feedback:** After receiving feedback from users, the agent updates its knowledge base. If the feedback is positive, the agent increases the rating of the recommended destination. If the feedback is negative, the rating is decreased. This helps the agent refine its recommendations based on real user experiences.\n",
    "\n",
    "### 4. **Making Recommendations:**\n",
    "   - **Calculating Scores:** The agent evaluates each destination by calculating a score based on its rating and, if eco-friendly options are a goal, it adjusts the score by adding the sustainability rating.\n",
    "   - **Selecting the Best Destination:** The destination with the highest score is recommended to the user. This process ensures that the recommendation aligns with both user preferences and the agentâ€™s goals.\n",
    "\n",
    "### 5. **Engaging with the User:**\n",
    "   - **Providing Recommendations:** The agent presents the recommended destination to the user and asks for feedback.\n",
    "   - **Feedback Handling:** The feedback (positive or negative) is used to update the knowledge base, which helps improve future recommendations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c261adf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectiveTravelAgentWithSelfModeling:\n",
    "    def __init__(self):\n",
    "        # Initialize the agent with a self-model that includes goals and a knowledge base\n",
    "        self.self_model = {\n",
    "            \"goals\": {\n",
    "                \"personalized_recommendations\": True,\n",
    "                \"optimize_user_satisfaction\": True,\n",
    "                \"eco_friendly_options\": False  # Default: Not prioritizing eco-friendly options\n",
    "            },\n",
    "            \"knowledge_base\": {\n",
    "                \"destinations\": {\n",
    "                    \"Paris\": {\"rating\": 4.8, \"cost\": 2000, \"luxury\": 0.9, \"sustainability\": 0.3},\n",
    "                    \"Bangkok\": {\"rating\": 4.5, \"cost\": 1500, \"luxury\": 0.7, \"sustainability\": 0.6},\n",
    "                    \"Barcelona\": {\"rating\": 4.7, \"cost\": 1800, \"luxury\": 0.8, \"sustainability\": 0.7}\n",
    "                },\n",
    "                \"user_preferences\": {}\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def update_goals(self, new_preferences):\n",
    "        \"\"\"Update the agent's goals based on new user preferences.\"\"\"\n",
    "        if new_preferences.get(\"eco_friendly\"):\n",
    "            self.self_model[\"goals\"][\"eco_friendly_options\"] = True\n",
    "            print(\"Updated goal: Prioritize eco-friendly travel options.\")\n",
    "        if new_preferences.get(\"adjust_budget\"):\n",
    "            print(\"Updated goal: Adjust travel options based on new budget constraints.\")\n",
    "    \n",
    "    def update_knowledge_base(self, feedback):\n",
    "        \"\"\"Update the agent's knowledge base based on user feedback.\"\"\"\n",
    "        destination = feedback[\"destination\"]\n",
    "        if feedback[\"positive\"]:\n",
    "            # Increase rating for positive feedback\n",
    "            self.self_model[\"knowledge_base\"][\"destinations\"][destination][\"rating\"] += 0.1\n",
    "            print(f\"Positive feedback received for {destination}; rating increased.\")\n",
    "        else:\n",
    "            # Decrease rating for negative feedback\n",
    "            self.self_model[\"knowledge_base\"][\"destinations\"][destination][\"rating\"] -= 0.2\n",
    "            print(f\"Negative feedback received for {destination}; rating decreased.\")\n",
    "    \n",
    "    def recommend_destination(self, user_preferences):\n",
    "        \"\"\"Recommend a destination based on user preferences and the agent's self-model.\"\"\"\n",
    "        # Store user preferences in the agent's self-model\n",
    "        self.self_model[\"knowledge_base\"][\"user_preferences\"] = user_preferences\n",
    "        \n",
    "        # Update agent's goals based on new preferences\n",
    "        if user_preferences.get(\"eco_friendly\"):\n",
    "            self.update_goals(user_preferences)\n",
    "        \n",
    "        # Calculate scores for each destination\n",
    "        best_destination = None\n",
    "        highest_score = 0\n",
    "        for destination, info in self.self_model[\"knowledge_base\"][\"destinations\"].items():\n",
    "            score = info[\"rating\"]\n",
    "            if self.self_model[\"goals\"][\"eco_friendly_options\"]:\n",
    "                # Boost score for eco-friendly options if that goal is prioritized\n",
    "                score += info[\"sustainability\"]\n",
    "            \n",
    "            # Update the best destination if current score is higher\n",
    "            if score > highest_score:\n",
    "                best_destination = destination\n",
    "                highest_score = score\n",
    "        \n",
    "        return best_destination\n",
    "\n",
    "    def engage_with_user(self, destination):\n",
    "        \"\"\"Simulate user engagement by providing the recommendation and receiving feedback.\"\"\"\n",
    "        print(f\"Recommended destination: {destination}\")\n",
    "        # Simulate receiving user feedback (e.g., through input in a real application)\n",
    "        feedback = input(f\"Did you like the recommendation of {destination}? (yes/no): \").strip().lower()\n",
    "        positive_feedback = feedback == \"yes\"\n",
    "        return {\"destination\": destination, \"positive\": positive_feedback}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f6fdea",
   "metadata": {},
   "source": [
    "The provided code snippet is designed to simulate the usage of the `ReflectiveTravelAgentWithSelfModeling` class. \n",
    "\n",
    "### 1. **Creating an Instance of the Agent:**\n",
    "   ```python\n",
    "   agent = ReflectiveTravelAgentWithSelfModeling()\n",
    "   ```\n",
    "   - **Purpose:** Initializes a new instance of the `ReflectiveTravelAgentWithSelfModeling` class.\n",
    "   - **Outcome:** This instance represents a travel agent equipped with self-modeling capabilities, including goal management and a knowledge base.\n",
    "\n",
    "### 2. **Setting User Preferences:**\n",
    "   ```python\n",
    "   user_preferences = {\n",
    "       \"budget\": 0.6,            # Moderate budget constraint\n",
    "       \"luxury\": 0.4,            # Moderate preference for luxury\n",
    "       \"adventure\": 0.7,         # High preference for adventure\n",
    "       \"eco_friendly\": True      # User prefers eco-friendly options\n",
    "   }\n",
    "   ```\n",
    "   - **Purpose:** Defines a set of preferences provided by the user.\n",
    "   - **Outcome:** These preferences indicate that the user has a moderate budget, moderate luxury preferences, a high interest in adventure, and a strong preference for eco-friendly options.\n",
    "\n",
    "### 3. **Getting a Recommendation:**\n",
    "   ```python\n",
    "   recommendation = agent.recommend_destination(user_preferences)\n",
    "   ```\n",
    "   - **Purpose:** Requests a travel destination recommendation from the agent based on the provided user preferences.\n",
    "   - **Outcome:** The agent processes the preferences, updates its goals if necessary (e.g., prioritizing eco-friendly options), and selects the best destination to recommend.\n",
    "\n",
    "### 4. **Engaging with the User:**\n",
    "   ```python\n",
    "   feedback = agent.engage_with_user(recommendation)\n",
    "   ```\n",
    "   - **Purpose:** Simulates interaction with the user by presenting the recommendation and gathering feedback.\n",
    "   - **Outcome:** The user provides feedback on the recommended destination, which is used to evaluate the effectiveness of the recommendation.\n",
    "\n",
    "### 5. **Updating the Knowledge Base:**\n",
    "   ```python\n",
    "   agent.update_knowledge_base(feedback)\n",
    "   ```\n",
    "   - **Purpose:** Updates the agentâ€™s knowledge base with the feedback received from the user.\n",
    "   - **Outcome:** The agent adjusts its knowledge base by modifying ratings or other attributes based on whether the feedback was positive or negative. This update helps improve future recommendations by refining the agent's understanding of user preferences and destination qualities.\n",
    "\n",
    "### Summary:\n",
    "In essence, this code snippet demonstrates how the `ReflectiveTravelAgentWithSelfModeling` class operates in a simulated environment. It initializes the agent, sets user preferences, obtains a recommendation, engages the user for feedback, and updates the agentâ€™s knowledge base based on that feedback. This simulation helps illustrate the agentâ€™s self-modeling capabilities and its ability to adapt and improve recommendations over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b66ea9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated goal: Prioritize eco-friendly travel options.\n",
      "Recommended destination: Barcelona\n",
      "Did you like the recommendation of Barcelona? (yes/no): no\n",
      "Negative feedback received for Barcelona; rating decreased.\n"
     ]
    }
   ],
   "source": [
    "# Simulating agent usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create an instance of the reflective travel agent with self-modeling\n",
    "    agent = ReflectiveTravelAgentWithSelfModeling()\n",
    "    \n",
    "    # Example user preferences including a focus on eco-friendly options\n",
    "    user_preferences = {\n",
    "        \"budget\": 0.6,            # Moderate budget constraint\n",
    "        \"luxury\": 0.4,            # Moderate preference for luxury\n",
    "        \"adventure\": 0.7,         # High preference for adventure\n",
    "        \"eco_friendly\": True      # User prefers eco-friendly options\n",
    "    }\n",
    "    \n",
    "    # Get the recommended destination based on user preferences\n",
    "    recommendation = agent.recommend_destination(user_preferences)\n",
    "    \n",
    "    # Engage with the user to provide feedback on the recommendation\n",
    "    feedback = agent.engage_with_user(recommendation)\n",
    "    \n",
    "    # Update the knowledge base with the user feedback\n",
    "    agent.update_knowledge_base(feedback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e3c2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
