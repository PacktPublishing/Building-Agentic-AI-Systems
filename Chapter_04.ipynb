{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "478113ec",
   "metadata": {},
   "source": [
    "# Meta Reasoning - example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac6d01cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcee2139",
   "metadata": {},
   "source": [
    "## Simulated travel agent with meta-reasoning capabilities\n",
    "\n",
    "- recommend_destination: The agent recommends a destination based on user preferences (budget, luxury, adventure) and internal weightings.\n",
    "\n",
    "- get_user_feedback: The agent receives feedback on the recommendation (positive or negative).\n",
    "\n",
    "- meta_reasoning: The agent adjusts its reasoning by updating the weights based on feedback, improving future recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efa6edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated travel agent with meta-reasoning capabilities\n",
    "class ReflectiveTravelAgent:\n",
    "    def __init__(self):\n",
    "        # Initialize preference weights that determine how user preferences influence recommendations\n",
    "        self.preferences_weights = {\n",
    "            \"budget\": 0.5,    # Weight for budget-related preferences\n",
    "            \"luxury\": 0.3,    # Weight for luxury-related preferences\n",
    "            \"adventure\": 0.2  # Weight for adventure-related preferences\n",
    "        }\n",
    "        self.user_feedback = []  # List to store user feedback for meta-reasoning\n",
    "\n",
    "    def recommend_destination(self, user_preferences):\n",
    "        \"\"\"\n",
    "        Recommend a destination based on user preferences and internal weightings.\n",
    "\n",
    "        Args:\n",
    "            user_preferences (dict): User's preferences with keys like 'budget', 'luxury', 'adventure'\n",
    "\n",
    "        Returns:\n",
    "            str: Recommended destination\n",
    "        \"\"\"\n",
    "        # Calculate scores for each destination based on weighted user preferences\n",
    "        score = {\n",
    "            \"Paris\": (self.preferences_weights[\"luxury\"] * user_preferences[\"luxury\"] + \n",
    "                      self.preferences_weights[\"adventure\"] * user_preferences[\"adventure\"]),\n",
    "            \"Bangkok\": (self.preferences_weights[\"budget\"] * user_preferences[\"budget\"] +\n",
    "                        self.preferences_weights[\"adventure\"] * user_preferences[\"adventure\"]),\n",
    "            \"New York\": (self.preferences_weights[\"luxury\"] * user_preferences[\"luxury\"] +\n",
    "                         self.preferences_weights[\"budget\"] * user_preferences[\"budget\"])\n",
    "        }\n",
    "        # Select the destination with the highest calculated score\n",
    "        recommendation = max(score, key=score.get)\n",
    "        return recommendation\n",
    "\n",
    "    def get_user_feedback(self, actual_experience):\n",
    "        \"\"\"\n",
    "        Simulate receiving user feedback and trigger meta-reasoning to adjust recommendations.\n",
    "\n",
    "        Args:\n",
    "            actual_experience (str): The destination the user experienced\n",
    "        \"\"\"\n",
    "        # Simulate user feedback: 1 for positive, -1 for negative\n",
    "        feedback = random.choice([1, -1])\n",
    "        print(f\"Feedback for {actual_experience}: {'Positive' if feedback == 1 else 'Negative'}\")\n",
    "        \n",
    "        # Store the feedback for later analysis\n",
    "        self.user_feedback.append((actual_experience, feedback))\n",
    "        \n",
    "        # Trigger meta-reasoning to adjust the agent's reasoning process based on feedback\n",
    "        self.meta_reasoning()\n",
    "\n",
    "    def meta_reasoning(self):\n",
    "        \"\"\"\n",
    "        Analyze collected feedback and adjust preference weights to improve future recommendations.\n",
    "        This simulates the agent reflecting on its reasoning process and making adjustments.\n",
    "        \"\"\"\n",
    "        for destination, feedback in self.user_feedback:\n",
    "            if feedback == -1:  # Negative feedback indicates dissatisfaction\n",
    "                # Reduce the weight of the main attribute associated with the destination\n",
    "                if destination == \"Paris\":\n",
    "                    self.preferences_weights[\"luxury\"] *= 0.9  # Decrease luxury preference\n",
    "                elif destination == \"Bangkok\":\n",
    "                    self.preferences_weights[\"budget\"] *= 0.9  # Decrease budget preference\n",
    "                elif destination == \"New York\":\n",
    "                    self.preferences_weights[\"budget\"] *= 0.9  # Decrease budget preference\n",
    "            elif feedback == 1:  # Positive feedback indicates satisfaction\n",
    "                # Increase the weight of the main attribute associated with the destination\n",
    "                if destination == \"Paris\":\n",
    "                    self.preferences_weights[\"luxury\"] *= 1.1  # Increase luxury preference\n",
    "                elif destination == \"Bangkok\":\n",
    "                    self.preferences_weights[\"budget\"] *= 1.1  # Increase budget preference\n",
    "                elif destination == \"New York\":\n",
    "                    self.preferences_weights[\"budget\"] *= 1.1  # Increase budget preference\n",
    "\n",
    "        # Normalize weights to ensure they sum up to 1 for consistency\n",
    "        total_weight = sum(self.preferences_weights.values())\n",
    "        for key in self.preferences_weights:\n",
    "            self.preferences_weights[key] /= total_weight\n",
    "\n",
    "        # Display updated weights after meta-reasoning adjustments\n",
    "        print(f\"Updated weights: {self.preferences_weights}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4ad2f0",
   "metadata": {},
   "source": [
    "## Simulation\n",
    "\n",
    "- User Preferences: Defines the user's preferences for budget, luxury, and adventure.\n",
    "\n",
    "- First Recommendation: The agent recommends a destination based on the initial weights and user preferences.\n",
    "\n",
    "- User Feedback Simulation: Simulates the user providing feedback on the recommended destination.\n",
    "\n",
    "- Second Recommendation: After adjusting the weights based on feedback, the agent makes a new recommendation that reflects the updated reasoning process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c29fc98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended destination: Bangkok\n",
      "Feedback for Bangkok: Positive\n",
      "Updated weights: {'budget': 0.5238095238095238, 'luxury': 0.2857142857142857, 'adventure': 0.19047619047619047}\n",
      "\n",
      "Updated recommendation: Bangkok\n"
     ]
    }
   ],
   "source": [
    "# Simulate agent usage\n",
    "if __name__ == \"__main__\":\n",
    "    agent = ReflectiveTravelAgent()\n",
    "\n",
    "    # User's initial preferences\n",
    "    user_preferences = {\n",
    "        \"budget\": 0.8,      # High preference for budget-friendly options\n",
    "        \"luxury\": 0.2,      # Low preference for luxury\n",
    "        \"adventure\": 0.5    # Moderate preference for adventure activities\n",
    "    }\n",
    "\n",
    "    # First recommendation based on initial preferences and weights\n",
    "    recommended = agent.recommend_destination(user_preferences)\n",
    "    print(f\"Recommended destination: {recommended}\")\n",
    "\n",
    "    # Simulate user experience and provide feedback\n",
    "    agent.get_user_feedback(recommended)\n",
    "\n",
    "    # Second recommendation after adjusting weights based on feedback\n",
    "    recommended = agent.recommend_destination(user_preferences)\n",
    "    print(f\"Updated recommendation: {recommended}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1104c35a",
   "metadata": {},
   "source": [
    "# Self Explanation - example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5dd7e0",
   "metadata": {},
   "source": [
    "## ReflectiveTravelAgentWithSelfExplanation\n",
    "\n",
    "The `ReflectiveTravelAgentWithSelfExplanation` class simulates a travel agent that not only recommends destinations based on user preferences but also explains the reasoning behind its recommendations. \n",
    "\n",
    "1. **Preference-Based Recommendations**: It takes user preferences (like budget, luxury, and adventure preferences) and calculates scores for different travel destinations by weighing those preferences. The destination with the highest score is recommended to the user.\n",
    "\n",
    "2. **Self-Explanation**: For each recommendation, the agent generates a detailed self-explanation. This explanation outlines the factors that led to the recommendation, such as proximity to popular attractions, budget-friendly options, or the presence of adventure activities. The purpose is to provide transparency into how the decision was made, helping the user understand the reasoning process.\n",
    "\n",
    "3. **Feedback Reflection**: The agent doesn't stop after making the recommendation. It actively reflects on user feedback (whether positive or negative). If the feedback is negative, it introspects on its decision-making process and adjusts the importance (weights) it assigns to user preferences for future recommendations. For instance, if a user dislikes a budget-friendly recommendation, the agent might reduce the emphasis it places on budget-related preferences.\n",
    "\n",
    "4. **User Engagement**: The class also simulates a dialogue with the user. After giving the recommendation and the self-explanation, it collects feedback from the user, allowing for a more collaborative interaction. This feedback is then used to refine future recommendations, making the agent more adaptive and personalized.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5ebd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectiveTravelAgentWithSelfExplanation:\n",
    "    def __init__(self):\n",
    "        # Initialize the internal weights for user preferences (e.g., budget, luxury, adventure)\n",
    "        self.preferences_weights = {\n",
    "            \"budget\": 0.4,    # Weight for budget-related preferences\n",
    "            \"luxury\": 0.3,    # Weight for luxury-related preferences\n",
    "            \"adventure\": 0.3  # Weight for adventure-related preferences\n",
    "        }\n",
    "\n",
    "    def recommend_destination(self, user_preferences):\n",
    "        \"\"\"\n",
    "        Recommend a destination based on user preferences and provide a self-explanation.\n",
    "\n",
    "        Args:\n",
    "            user_preferences (dict): User's preferences for different factors (e.g., budget, luxury, adventure)\n",
    "        \n",
    "        Returns:\n",
    "            (str, str): Recommended destination and the self-explanation\n",
    "        \"\"\"\n",
    "        # Score each destination by multiplying preference weights with user preferences\n",
    "        score = {\n",
    "            \"Paris\": (self.preferences_weights[\"luxury\"] * user_preferences[\"luxury\"] + \n",
    "                      self.preferences_weights[\"adventure\"] * user_preferences[\"adventure\"]),\n",
    "            \"Bangkok\": (self.preferences_weights[\"budget\"] * user_preferences[\"budget\"] +\n",
    "                        self.preferences_weights[\"adventure\"] * user_preferences[\"adventure\"]),\n",
    "            \"New York\": (self.preferences_weights[\"luxury\"] * user_preferences[\"luxury\"] +\n",
    "                         self.preferences_weights[\"budget\"] * user_preferences[\"budget\"])\n",
    "        }\n",
    "        \n",
    "        # Choose the destination with the highest score\n",
    "        recommendation = max(score, key=score.get)\n",
    "        \n",
    "        # Generate and return a self-explanation for the recommendation\n",
    "        explanation = self.generate_self_explanation(recommendation, score[recommendation], user_preferences)\n",
    "        \n",
    "        return recommendation, explanation\n",
    "\n",
    "    def generate_self_explanation(self, destination, score, user_preferences):\n",
    "        \"\"\"\n",
    "        Generate a self-explanation for the recommended destination.\n",
    "        \n",
    "        Args:\n",
    "            destination (str): The recommended destination\n",
    "            score (float): The score assigned to the destination\n",
    "            user_preferences (dict): The user's preferences used for the recommendation\n",
    "        \n",
    "        Returns:\n",
    "            str: Self-explanation of the recommendation\n",
    "        \"\"\"\n",
    "        # Start the explanation with the destination and its score\n",
    "        explanation = (\n",
    "            f\"I recommended {destination} because it aligns with your preferences. \"\n",
    "            f\"The destination scored {score:.2f} based on the following factors:\\n\"\n",
    "        )\n",
    "        \n",
    "        # Customize the explanation for each destination based on user preferences\n",
    "        if destination == \"Paris\":\n",
    "            explanation += (\n",
    "                \"- High luxury offerings (aligned with your luxury preference).\\n\"\n",
    "                \"- Availability of adventure activities.\\n\"\n",
    "            )\n",
    "        elif destination == \"Bangkok\":\n",
    "            explanation += (\n",
    "                \"- Budget-friendly options (aligned with your budget preference).\\n\"\n",
    "                \"- Availability of adventure experiences.\\n\"\n",
    "            )\n",
    "        elif destination == \"New York\":\n",
    "            explanation += (\n",
    "                \"- Combination of luxury experiences and budget-friendly options.\\n\"\n",
    "            )\n",
    "        \n",
    "        return explanation\n",
    "\n",
    "    def reflect_on_feedback(self, destination, user_feedback):\n",
    "        \"\"\"\n",
    "        Reflect on user feedback to improve decision-making in future recommendations.\n",
    "        \n",
    "        Args:\n",
    "            destination (str): The destination that was recommended\n",
    "            user_feedback (str): User feedback ('positive' or 'negative')\n",
    "        \"\"\"\n",
    "        # If the user provides negative feedback, adjust the internal reasoning process\n",
    "        if user_feedback == 'negative':\n",
    "            print(f\"User provided negative feedback for {destination}. Reflecting on reasoning...\")\n",
    "            \n",
    "            # Example: If Bangkok was chosen and the user disliked it, reduce budget weight\n",
    "            if destination == \"Bangkok\":\n",
    "                print(\"Realizing that budget weight might have been overemphasized. Reconsidering weights...\")\n",
    "                self.preferences_weights[\"budget\"] *= 0.9  # Reduce budget importance slightly\n",
    "\n",
    "            # If Paris, reduce importance of luxury if feedback is negative\n",
    "            elif destination == \"Paris\":\n",
    "                print(\"Luxury might have been over-prioritized. Adjusting luxury weight...\")\n",
    "                self.preferences_weights[\"luxury\"] *= 0.9\n",
    "\n",
    "            # Normalize weights after adjustment to maintain balance\n",
    "            total_weight = sum(self.preferences_weights.values())\n",
    "            for key in self.preferences_weights:\n",
    "                self.preferences_weights[key] /= total_weight  # Normalize weights\n",
    "\n",
    "            print(f\"Updated weights: {self.preferences_weights}\\n\")\n",
    "        else:\n",
    "            # Positive feedback indicates no changes are needed\n",
    "            print(f\"User provided positive feedback for {destination}. No changes needed.\")\n",
    "\n",
    "    def engage_with_user(self, recommendation, explanation):\n",
    "        \"\"\"\n",
    "        Simulate user interaction by providing a self-explanation and inviting feedback.\n",
    "\n",
    "        Args:\n",
    "            recommendation (str): The recommended destination\n",
    "            explanation (str): Self-explanation for the recommendation\n",
    "        \"\"\"\n",
    "        # Show the recommendation and its explanation to the user\n",
    "        print(f\"Recommended destination: {recommendation}\")\n",
    "        print(f\"Self-explanation: {explanation}\")\n",
    "\n",
    "        # Simulate user feedback (positive or negative)\n",
    "        user_feedback = input(f\"Did you like the recommendation for {recommendation}? (positive/negative): \")\n",
    "        \n",
    "        # Reflect on feedback and adjust the agent's reasoning process if needed\n",
    "        self.reflect_on_feedback(recommendation, user_feedback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb70b41",
   "metadata": {},
   "source": [
    "The provided code below simulates the usage of the `ReflectiveTravelAgentWithSelfExplanation` class, showcasing how the travel agent interacts with the user. \n",
    "\n",
    "1. **Agent Creation**: It first creates an instance of the `ReflectiveTravelAgentWithSelfExplanation` class, which initializes the agent with predefined weights for user preferences like budget, luxury, and adventure.\n",
    "\n",
    "2. **User Preferences**: It defines a sample user's travel preferences. In this case, the user has a high preference for budget-friendly options (`budget: 0.7`), a low preference for luxury experiences (`luxury: 0.2`), and a moderate preference for adventure activities (`adventure: 0.6`).\n",
    "\n",
    "3. **Generate Recommendation and Self-Explanation**: The agent uses the `recommend_destination` method to recommend a travel destination based on these preferences. Along with the recommendation, the agent also generates a self-explanation, which describes why the particular destination was chosen.\n",
    "\n",
    "4. **User Engagement and Feedback**: The agent then engages with the user by displaying the recommended destination and its explanation. Afterward, it collects feedback from the user (whether they liked the recommendation or not) and uses that feedback to reflect on its decision-making process, adjusting its internal reasoning if necessary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfb5337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended destination: Bangkok\n",
      "Self-explanation: I recommended Bangkok because it aligns with your preferences. The destination scored 0.46 based on the following factors:\n",
      "- Budget-friendly options (aligned with your budget preference).\n",
      "- Availability of adventure experiences.\n",
      "\n",
      "Did you like the recommendation for Bangkok? (positive/negative): negative\n",
      "User provided negative feedback for Bangkok. Reflecting on reasoning...\n",
      "Realizing that budget weight might have been overemphasized. Reconsidering weights...\n",
      "Updated weights: {'budget': 0.37500000000000006, 'luxury': 0.3125, 'adventure': 0.3125}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Simulating agent usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the reflective travel agent with self-explanation\n",
    "    agent = ReflectiveTravelAgentWithSelfExplanation()\n",
    "    \n",
    "    # Example: User's preferences (high budget preference, low luxury, moderate adventure)\n",
    "    user_preferences = {\n",
    "        \"budget\": 0.7,      # High preference for budget-friendly options\n",
    "        \"luxury\": 0.2,      # Low preference for luxury\n",
    "        \"adventure\": 0.6    # Moderate preference for adventure activities\n",
    "    }\n",
    "    \n",
    "    # Get the destination recommendation and self-explanation\n",
    "    recommendation, explanation = agent.recommend_destination(user_preferences)\n",
    "    \n",
    "    # Engage with the user by providing the recommendation, explanation, and receiving feedback\n",
    "    agent.engage_with_user(recommendation, explanation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f01747",
   "metadata": {},
   "source": [
    "# Self Modeling - example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e14cd4",
   "metadata": {},
   "source": [
    "The `ReflectiveTravelAgentWithSelfModeling` class represents a sophisticated travel recommendation system that utilizes **self-modeling** to enhance its decision-making and adaptability. \n",
    "\n",
    "### 1. **Initialization:**\n",
    "   - **Self-Model and Knowledge Base:** The agent starts with an internal model that includes its goals and a knowledge base. \n",
    "     - **Goals:** Initially, the goals are set to provide personalized recommendations, optimize user satisfaction, and not prioritize eco-friendly options by default.\n",
    "     - **Knowledge Base:** It contains information about various travel destinations, including their ratings, costs, luxury levels, and sustainability. This base also tracks user preferences.\n",
    "\n",
    "### 2. **Updating Goals:**\n",
    "   - **Adapting to Preferences:** When new user preferences are provided, the agent can update its goals accordingly. For example, if the user prefers eco-friendly options, the agent will adjust its goals to prioritize recommending sustainable travel options. Similarly, if the user’s budget changes, the agent will refocus on cost-effective recommendations.\n",
    "\n",
    "### 3. **Updating Knowledge Base:**\n",
    "   - **Incorporating Feedback:** After receiving feedback from users, the agent updates its knowledge base. If the feedback is positive, the agent increases the rating of the recommended destination. If the feedback is negative, the rating is decreased. This helps the agent refine its recommendations based on real user experiences.\n",
    "\n",
    "### 4. **Making Recommendations:**\n",
    "   - **Calculating Scores:** The agent evaluates each destination by calculating a score based on its rating and, if eco-friendly options are a goal, it adjusts the score by adding the sustainability rating.\n",
    "   - **Selecting the Best Destination:** The destination with the highest score is recommended to the user. This process ensures that the recommendation aligns with both user preferences and the agent’s goals.\n",
    "\n",
    "### 5. **Engaging with the User:**\n",
    "   - **Providing Recommendations:** The agent presents the recommended destination to the user and asks for feedback.\n",
    "   - **Feedback Handling:** The feedback (positive or negative) is used to update the knowledge base, which helps improve future recommendations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c261adf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectiveTravelAgentWithSelfModeling:\n",
    "    def __init__(self):\n",
    "        # Initialize the agent with a self-model that includes goals and a knowledge base\n",
    "        self.self_model = {\n",
    "            \"goals\": {\n",
    "                \"personalized_recommendations\": True,\n",
    "                \"optimize_user_satisfaction\": True,\n",
    "                \"eco_friendly_options\": False  # Default: Not prioritizing eco-friendly options\n",
    "            },\n",
    "            \"knowledge_base\": {\n",
    "                \"destinations\": {\n",
    "                    \"Paris\": {\"rating\": 4.8, \"cost\": 2000, \"luxury\": 0.9, \"sustainability\": 0.3},\n",
    "                    \"Bangkok\": {\"rating\": 4.5, \"cost\": 1500, \"luxury\": 0.7, \"sustainability\": 0.6},\n",
    "                    \"Barcelona\": {\"rating\": 4.7, \"cost\": 1800, \"luxury\": 0.8, \"sustainability\": 0.7}\n",
    "                },\n",
    "                \"user_preferences\": {}\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def update_goals(self, new_preferences):\n",
    "        \"\"\"Update the agent's goals based on new user preferences.\"\"\"\n",
    "        if new_preferences.get(\"eco_friendly\"):\n",
    "            self.self_model[\"goals\"][\"eco_friendly_options\"] = True\n",
    "            print(\"Updated goal: Prioritize eco-friendly travel options.\")\n",
    "        if new_preferences.get(\"adjust_budget\"):\n",
    "            print(\"Updated goal: Adjust travel options based on new budget constraints.\")\n",
    "    \n",
    "    def update_knowledge_base(self, feedback):\n",
    "        \"\"\"Update the agent's knowledge base based on user feedback.\"\"\"\n",
    "        destination = feedback[\"destination\"]\n",
    "        if feedback[\"positive\"]:\n",
    "            # Increase rating for positive feedback\n",
    "            self.self_model[\"knowledge_base\"][\"destinations\"][destination][\"rating\"] += 0.1\n",
    "            print(f\"Positive feedback received for {destination}; rating increased.\")\n",
    "        else:\n",
    "            # Decrease rating for negative feedback\n",
    "            self.self_model[\"knowledge_base\"][\"destinations\"][destination][\"rating\"] -= 0.2\n",
    "            print(f\"Negative feedback received for {destination}; rating decreased.\")\n",
    "    \n",
    "    def recommend_destination(self, user_preferences):\n",
    "        \"\"\"Recommend a destination based on user preferences and the agent's self-model.\"\"\"\n",
    "        # Store user preferences in the agent's self-model\n",
    "        self.self_model[\"knowledge_base\"][\"user_preferences\"] = user_preferences\n",
    "        \n",
    "        # Update agent's goals based on new preferences\n",
    "        if user_preferences.get(\"eco_friendly\"):\n",
    "            self.update_goals(user_preferences)\n",
    "        \n",
    "        # Calculate scores for each destination\n",
    "        best_destination = None\n",
    "        highest_score = 0\n",
    "        for destination, info in self.self_model[\"knowledge_base\"][\"destinations\"].items():\n",
    "            score = info[\"rating\"]\n",
    "            if self.self_model[\"goals\"][\"eco_friendly_options\"]:\n",
    "                # Boost score for eco-friendly options if that goal is prioritized\n",
    "                score += info[\"sustainability\"]\n",
    "            \n",
    "            # Update the best destination if current score is higher\n",
    "            if score > highest_score:\n",
    "                best_destination = destination\n",
    "                highest_score = score\n",
    "        \n",
    "        return best_destination\n",
    "\n",
    "    def engage_with_user(self, destination):\n",
    "        \"\"\"Simulate user engagement by providing the recommendation and receiving feedback.\"\"\"\n",
    "        print(f\"Recommended destination: {destination}\")\n",
    "        # Simulate receiving user feedback (e.g., through input in a real application)\n",
    "        feedback = input(f\"Did you like the recommendation of {destination}? (yes/no): \").strip().lower()\n",
    "        positive_feedback = feedback == \"yes\"\n",
    "        return {\"destination\": destination, \"positive\": positive_feedback}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f6fdea",
   "metadata": {},
   "source": [
    "The provided code snippet is designed to simulate the usage of the `ReflectiveTravelAgentWithSelfModeling` class. \n",
    "\n",
    "### 1. **Creating an Instance of the Agent:**\n",
    "   ```python\n",
    "   agent = ReflectiveTravelAgentWithSelfModeling()\n",
    "   ```\n",
    "   - **Purpose:** Initializes a new instance of the `ReflectiveTravelAgentWithSelfModeling` class.\n",
    "   - **Outcome:** This instance represents a travel agent equipped with self-modeling capabilities, including goal management and a knowledge base.\n",
    "\n",
    "### 2. **Setting User Preferences:**\n",
    "   ```python\n",
    "   user_preferences = {\n",
    "       \"budget\": 0.6,            # Moderate budget constraint\n",
    "       \"luxury\": 0.4,            # Moderate preference for luxury\n",
    "       \"adventure\": 0.7,         # High preference for adventure\n",
    "       \"eco_friendly\": True      # User prefers eco-friendly options\n",
    "   }\n",
    "   ```\n",
    "   - **Purpose:** Defines a set of preferences provided by the user.\n",
    "   - **Outcome:** These preferences indicate that the user has a moderate budget, moderate luxury preferences, a high interest in adventure, and a strong preference for eco-friendly options.\n",
    "\n",
    "### 3. **Getting a Recommendation:**\n",
    "   ```python\n",
    "   recommendation = agent.recommend_destination(user_preferences)\n",
    "   ```\n",
    "   - **Purpose:** Requests a travel destination recommendation from the agent based on the provided user preferences.\n",
    "   - **Outcome:** The agent processes the preferences, updates its goals if necessary (e.g., prioritizing eco-friendly options), and selects the best destination to recommend.\n",
    "\n",
    "### 4. **Engaging with the User:**\n",
    "   ```python\n",
    "   feedback = agent.engage_with_user(recommendation)\n",
    "   ```\n",
    "   - **Purpose:** Simulates interaction with the user by presenting the recommendation and gathering feedback.\n",
    "   - **Outcome:** The user provides feedback on the recommended destination, which is used to evaluate the effectiveness of the recommendation.\n",
    "\n",
    "### 5. **Updating the Knowledge Base:**\n",
    "   ```python\n",
    "   agent.update_knowledge_base(feedback)\n",
    "   ```\n",
    "   - **Purpose:** Updates the agent’s knowledge base with the feedback received from the user.\n",
    "   - **Outcome:** The agent adjusts its knowledge base by modifying ratings or other attributes based on whether the feedback was positive or negative. This update helps improve future recommendations by refining the agent's understanding of user preferences and destination qualities.\n",
    "\n",
    "### Summary:\n",
    "In essence, this code snippet demonstrates how the `ReflectiveTravelAgentWithSelfModeling` class operates in a simulated environment. It initializes the agent, sets user preferences, obtains a recommendation, engages the user for feedback, and updates the agent’s knowledge base based on that feedback. This simulation helps illustrate the agent’s self-modeling capabilities and its ability to adapt and improve recommendations over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b66ea9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated goal: Prioritize eco-friendly travel options.\n",
      "Recommended destination: Barcelona\n",
      "Did you like the recommendation of Barcelona? (yes/no): no\n",
      "Negative feedback received for Barcelona; rating decreased.\n"
     ]
    }
   ],
   "source": [
    "# Simulating agent usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create an instance of the reflective travel agent with self-modeling\n",
    "    agent = ReflectiveTravelAgentWithSelfModeling()\n",
    "    \n",
    "    # Example user preferences including a focus on eco-friendly options\n",
    "    user_preferences = {\n",
    "        \"budget\": 0.6,            # Moderate budget constraint\n",
    "        \"luxury\": 0.4,            # Moderate preference for luxury\n",
    "        \"adventure\": 0.7,         # High preference for adventure\n",
    "        \"eco_friendly\": True      # User prefers eco-friendly options\n",
    "    }\n",
    "    \n",
    "    # Get the recommended destination based on user preferences\n",
    "    recommendation = agent.recommend_destination(user_preferences)\n",
    "    \n",
    "    # Engage with the user to provide feedback on the recommendation\n",
    "    feedback = agent.engage_with_user(recommendation)\n",
    "    \n",
    "    # Update the knowledge base with the user feedback\n",
    "    agent.update_knowledge_base(feedback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e3c2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
