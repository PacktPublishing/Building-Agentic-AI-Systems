{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "478113ec",
   "metadata": {},
   "source": [
    "# Meta Reasoning - example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac6d01cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcee2139",
   "metadata": {},
   "source": [
    "## Simulated travel agent with meta-reasoning capabilities\n",
    "\n",
    "- recommend_destination: The agent recommends a destination based on user preferences (budget, luxury, adventure) and internal weightings.\n",
    "\n",
    "- get_user_feedback: The agent receives feedback on the recommendation (positive or negative).\n",
    "\n",
    "- meta_reasoning: The agent adjusts its reasoning by updating the weights based on feedback, improving future recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efa6edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated travel agent with meta-reasoning capabilities\n",
    "class ReflectiveTravelAgent:\n",
    "    def __init__(self):\n",
    "        # Initialize preference weights that determine how user preferences influence recommendations\n",
    "        self.preferences_weights = {\n",
    "            \"budget\": 0.5,    # Weight for budget-related preferences\n",
    "            \"luxury\": 0.3,    # Weight for luxury-related preferences\n",
    "            \"adventure\": 0.2  # Weight for adventure-related preferences\n",
    "        }\n",
    "        self.user_feedback = []  # List to store user feedback for meta-reasoning\n",
    "\n",
    "    def recommend_destination(self, user_preferences):\n",
    "        \"\"\"\n",
    "        Recommend a destination based on user preferences and internal weightings.\n",
    "\n",
    "        Args:\n",
    "            user_preferences (dict): User's preferences with keys like 'budget', 'luxury', 'adventure'\n",
    "\n",
    "        Returns:\n",
    "            str: Recommended destination\n",
    "        \"\"\"\n",
    "        # Calculate scores for each destination based on weighted user preferences\n",
    "        score = {\n",
    "            \"Paris\": (self.preferences_weights[\"luxury\"] * user_preferences[\"luxury\"] + \n",
    "                      self.preferences_weights[\"adventure\"] * user_preferences[\"adventure\"]),\n",
    "            \"Bangkok\": (self.preferences_weights[\"budget\"] * user_preferences[\"budget\"] +\n",
    "                        self.preferences_weights[\"adventure\"] * user_preferences[\"adventure\"]),\n",
    "            \"New York\": (self.preferences_weights[\"luxury\"] * user_preferences[\"luxury\"] +\n",
    "                         self.preferences_weights[\"budget\"] * user_preferences[\"budget\"])\n",
    "        }\n",
    "        # Select the destination with the highest calculated score\n",
    "        recommendation = max(score, key=score.get)\n",
    "        return recommendation\n",
    "\n",
    "    def get_user_feedback(self, actual_experience):\n",
    "        \"\"\"\n",
    "        Simulate receiving user feedback and trigger meta-reasoning to adjust recommendations.\n",
    "\n",
    "        Args:\n",
    "            actual_experience (str): The destination the user experienced\n",
    "        \"\"\"\n",
    "        # Simulate user feedback: 1 for positive, -1 for negative\n",
    "        feedback = random.choice([1, -1])\n",
    "        print(f\"Feedback for {actual_experience}: {'Positive' if feedback == 1 else 'Negative'}\")\n",
    "        \n",
    "        # Store the feedback for later analysis\n",
    "        self.user_feedback.append((actual_experience, feedback))\n",
    "        \n",
    "        # Trigger meta-reasoning to adjust the agent's reasoning process based on feedback\n",
    "        self.meta_reasoning()\n",
    "\n",
    "    def meta_reasoning(self):\n",
    "        \"\"\"\n",
    "        Analyze collected feedback and adjust preference weights to improve future recommendations.\n",
    "        This simulates the agent reflecting on its reasoning process and making adjustments.\n",
    "        \"\"\"\n",
    "        for destination, feedback in self.user_feedback:\n",
    "            if feedback == -1:  # Negative feedback indicates dissatisfaction\n",
    "                # Reduce the weight of the main attribute associated with the destination\n",
    "                if destination == \"Paris\":\n",
    "                    self.preferences_weights[\"luxury\"] *= 0.9  # Decrease luxury preference\n",
    "                elif destination == \"Bangkok\":\n",
    "                    self.preferences_weights[\"budget\"] *= 0.9  # Decrease budget preference\n",
    "                elif destination == \"New York\":\n",
    "                    self.preferences_weights[\"budget\"] *= 0.9  # Decrease budget preference\n",
    "            elif feedback == 1:  # Positive feedback indicates satisfaction\n",
    "                # Increase the weight of the main attribute associated with the destination\n",
    "                if destination == \"Paris\":\n",
    "                    self.preferences_weights[\"luxury\"] *= 1.1  # Increase luxury preference\n",
    "                elif destination == \"Bangkok\":\n",
    "                    self.preferences_weights[\"budget\"] *= 1.1  # Increase budget preference\n",
    "                elif destination == \"New York\":\n",
    "                    self.preferences_weights[\"budget\"] *= 1.1  # Increase budget preference\n",
    "\n",
    "        # Normalize weights to ensure they sum up to 1 for consistency\n",
    "        total_weight = sum(self.preferences_weights.values())\n",
    "        for key in self.preferences_weights:\n",
    "            self.preferences_weights[key] /= total_weight\n",
    "\n",
    "        # Display updated weights after meta-reasoning adjustments\n",
    "        print(f\"Updated weights: {self.preferences_weights}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4ad2f0",
   "metadata": {},
   "source": [
    "## Simulation\n",
    "\n",
    "- User Preferences: Defines the user's preferences for budget, luxury, and adventure.\n",
    "\n",
    "- First Recommendation: The agent recommends a destination based on the initial weights and user preferences.\n",
    "\n",
    "- User Feedback Simulation: Simulates the user providing feedback on the recommended destination.\n",
    "\n",
    "- Second Recommendation: After adjusting the weights based on feedback, the agent makes a new recommendation that reflects the updated reasoning process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c29fc98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended destination: Bangkok\n",
      "Feedback for Bangkok: Positive\n",
      "Updated weights: {'budget': 0.5238095238095238, 'luxury': 0.2857142857142857, 'adventure': 0.19047619047619047}\n",
      "\n",
      "Updated recommendation: Bangkok\n"
     ]
    }
   ],
   "source": [
    "# Simulate agent usage\n",
    "if __name__ == \"__main__\":\n",
    "    agent = ReflectiveTravelAgent()\n",
    "\n",
    "    # User's initial preferences\n",
    "    user_preferences = {\n",
    "        \"budget\": 0.8,      # High preference for budget-friendly options\n",
    "        \"luxury\": 0.2,      # Low preference for luxury\n",
    "        \"adventure\": 0.5    # Moderate preference for adventure activities\n",
    "    }\n",
    "\n",
    "    # First recommendation based on initial preferences and weights\n",
    "    recommended = agent.recommend_destination(user_preferences)\n",
    "    print(f\"Recommended destination: {recommended}\")\n",
    "\n",
    "    # Simulate user experience and provide feedback\n",
    "    agent.get_user_feedback(recommended)\n",
    "\n",
    "    # Second recommendation after adjusting weights based on feedback\n",
    "    recommended = agent.recommend_destination(user_preferences)\n",
    "    print(f\"Updated recommendation: {recommended}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c986da09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f86abc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95f3ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5652d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a255e9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daf37c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
